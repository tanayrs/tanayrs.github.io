<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Tanay Raghunandan Srinivasa — Project: LARS-UAV</title>
  <link rel="stylesheet" href="../assets/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&display=swap" rel="stylesheet">
  <style>
    header {
      display: flex;
      align-items: flex-start;
      gap: 1rem;
      margin-bottom: 2rem;
    }
    header h1 {
      font-size: 1.8rem;
      line-height: 1.4;
      margin: 0;
      flex: 1;
      word-wrap: break-word;
      overflow-wrap: break-word;
    }
    img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 1rem auto;
    }
    .back-home-btn {
      display: inline-block;
      width: 40px;
      height: 40px;
      background: var(--accent-color);
      color: var(--bg-color);
      text-decoration: none;
      border-radius: 50%;
      font-size: 1.2rem;
      font-weight: 500;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: all 0.3s ease;
      flex-shrink: 0;
    }
    .back-home-btn:hover {
      background: var(--text-color);
      transform: translateY(-1px);
      box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }
    @media (max-width: 768px) {
      header { flex-direction: column; align-items: flex-start; gap: 0.8rem; }
      header h1 { font-size: 1.5rem; line-height: 1.3; width: 100%; }
      section h2 { font-size: 1.3rem; }
      .back-home-btn { width: 36px; height: 36px; font-size: 1.1rem; }
    }
    @media (max-width: 480px) {
      header { gap: 0.6rem; }
      header h1 { font-size: 1.3rem; line-height: 1.2; width: 100%; }
      section h2 { font-size: 1.1rem; }
      .back-home-btn { width: 32px; height: 32px; font-size: 1rem; }
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <a href="../index.html" class="back-home-btn">←</a>
      <h1>LARS-UAV: Low-Altitude Remote Sensing using Unmanned Aerial Vehicles for Crop Health Monitoring</h1>
    </header>

    <section>
      <h2>Introduction</h2>
      <p>Agriculture is the backbone of India, with 50% of the nation depending on it for their livelihood.<sup>[1]</sup></p>
      <p>India produces 25% of the world's crop, however as per the Food and Agriculture Organization, 40% of the global crop production is lost every year due to pests and diseases.<sup>[2]</sup></p>
      <p><strong>Current Solution:</strong> Manual inspection by trained professionals, however the progress is slow.</p>
      <p><strong>Problem Statement:</strong> How can we remotely and autonomously monitor crop health, yield, diseases, and pests at various growth stages?</p>
      <p><strong>What Makes us Different:</strong></p>
      <ul>
        <li>Autonomous crop monitoring framework powered by low-altitude multispectral UAV imaging and on-device inference.</li>
        <li>Detection of stress zones, early disease symptoms, crop health, and deployment of ground robots for detailed inspection.</li>
      </ul>
    </section>

    <section>
      <h2>Motivation</h2>
      <p align="center">
        <img src="https://github.com/user-attachments/assets/f4088461-42d8-403c-a822-22eaa5f85e35" alt="NDVI maps"/>
      </p>
      <p><em>NDVI maps calculated for rice plot 6 (Quinta do Canal) using data from Sentinel-2A (left) and UAS (right) imagery the spatial resolution of, respectively, 10 m and 0.074 m. The data were obtained during the rice reproductive phase, respectively, on the 8th and 9th of July, 2020.</em></p>
      <p><em>Advantages of drone-based imaging to satellites:</em></p>
      <ul>
        <li>Satellite data has much lower resolution than UAS drones.</li>
        <li>UAS allows data collection at flexible timings.</li>
      </ul>
      <p><strong>Limitations:</strong> Imaging resolution is too low to monitor each plant.</p>
    </section>

    <section>
      <h2>How to monitor crop health?</h2>
      <h3>Our Paper Title</h3>
      <p>Advancing Real-Time Crop Disease Detection on Edge Computing Devices using Lightweight Convolutional Neural Networks</p>
      <h3>Gap</h3>
      <p>Early rice disease detection is crucial for food security, but deploying deep learning models in resource-constrained environments is challenging.</p>
      <h3>Our Approach</h3>
      <p>Early detection of rice diseases is vital for food security, but conventional deep learning models are too resource-intensive for real-time use on edge devices. We developed a lightweight convolutional neural network based on MobileNet-V4, trained on a dataset of 16,225 images covering 13 rice diseases. The model was optimized and converted to TensorFlow Lite (TFLite) format for efficient deployment on resource-constrained hardware like Raspberry Pi 5, Nvidia P100 GPUs, and CPUs. This enables fast, accurate disease detection directly on drones or field devices, bridging the gap between high-accuracy models and real-time field deployment, offering a practical solution for early rice disease detection in resource-limited environments.</p>
      <h3>Our Results</h3>
      <p align="center">
        <img src="https://github.com/user-attachments/assets/a0ff0660-9a50-4355-a834-1d360baf7d46" alt="results"/>
      </p>
      <p>While comparing baseline models, without any optimization, ResNet-34 performs marginally better than MobileNet-V4, and has significant gains over MobileNet-V2 and MobileNet-V3 as seen in Table 3.</p>
      <p>As seen in Table 4, MobileNet-V4 achieves a test accuracy of 97.84 % when optimized, demonstrating the eﬃcacy of these techniques in improving generalization over its baseline configuration and ResNet-34. Despite this high accuracy, the confusion matrix in Fig.3 reveals systematic misclassifications in visually similar disease categories, such as bacterial leaf blight vs. bacterial leaf streak and white stem borer vs. yellow stem borer. Additionally, minority classes, like bacterial panicle blight, exhibit slightly lower recall, suggesting the eﬀect of class imbalance. The model also misclassifies mild cases of brown spot and downy mildew as Normal, indicating potential overlap in visual features. This can be addressed by enhancing fine-grained disease diﬀerentiation through attention-based feature extraction.</p>
      <p>As shown in Table 5, TFLite lacks CUDA GPU optimizations for MobileNet-V4 and ResNet34, resulting in inference times that remain largely unchanged. In contrast, MobileNet-V2 experiences a significant performance boost. The observations made from Table 5 are as follows.</p>
      <ul>
        <li>MobileNet Models: Consistently demonstrated faster inference times across all platforms, particularly on the Raspberry Pi 5, due to their lightweight design.</li>
        <li>MobileNet-V4: Achieved the highest accuracy and the lowest inference times.</li>
        <li>TFLite Eﬃciency: TFLite models generally outperformed PyTorch models in inference speed, highlighting the advantages of the format for edge deployment.</li>
      </ul>
    
    <p>The experiments reveal critical trade-oﬀs between accuracy and speed. MobileNet-V4 emerged as a strong candidate for edge deployment, balancing speed and accuracy eﬀectively. It is also suited for scenarios, where computational resources are less constrained as it can provide even faster inference times. TFLite’s performance gains emphasize on the importance of model optimization for resource-limited platforms like the Raspberry Pi 5.</p>

    <p>The power eﬃciency analysis revealed that MobileNet-V4 demonstrated superior energy eﬃciency compared to other tested models, which can be seen in Table 6. MobileNet-V2 consumed approximately 30% less energy per inference than MobileNet-V4 and 90% less than ResNet34. However, this improved energy eﬃciency came at the cost of accuracy, as shown in Table 3. MobileNet-V4 presented a more balanced option, using significantly less energy than ResNet34 (66% reduction in energy per inference) while having a higher accuracy and F1-Score as mentioned in Table 3. When combined with the faster inference times reported in Table 5, MobileNet-V4 emerged as the optimal model for deployment in resource-constrained agricultural environments, where battery life and thermal management were critical considerations. The power eﬃciency analysis revealed that MobileNet-V4 demonstrated superior energy eﬃciency compared to other tested models, which can be seen in Table 6.</p>

    <h3>Paper Citation</h3>
    <p>Nanda, T.R., Shukla, A., Srinivasa, T.R., Bhargava, J., Chauhan, S. (2025). Advancing Real-Time Crop Disease Detection on Edge Computing Devices Using Lightweight Convolutional Neural Networks. In: Arai, K. (eds) Intelligent Systems and Applications. IntelliSys 2025. Lecture Notes in Networks and Systems, vol 1567. Springer, Cham. <a href="https://doi.org/10.1007/978-3-032-00071-233">https://doi.org/10.1007/978-3-032-00071-233</a></p>
    </section>

    <section>
      <h2>What about the groundbot?</h2>
      <p align="center">
        <img src="https://github.com/user-attachments/assets/ff40d9b7-80ba-491a-8e08-96dc5befac27" alt="Deployment Flowchart"/>
      </p>
      <p align="center">
        <img src="https://github.com/user-attachments/assets/c54e2d43-6401-447c-b71d-3fb3c8fe16d5" alt="ROS Environment"/>
      </p>
      <p>Figures show the deployment flowchart and ROS Environment overview.</p>
    </section>

    <section>
      <h2>GitHub</h2>
      <p><a href="https://github.com/lars-uav">https://github.com/lars-uav</a></p>
    </section>

    <section>
      <h2>Citations</h2>
      <ol>
        <li>Food and Agriculture Organization of the United Nations, “India at a glance,” FAO in India. [Online]. Available: <a href="https://www.fao.org/india/fao-in-india/india-at-a-glance/en/">FAO India at a glance</a></li>
        <li>Food and Agriculture Organization of the United Nations, “About Plant Production and Protection,” FAO. [Online]. Available: <a href="https://www.fao.org/plant-production-protection/about/en#:~:text=Every%20year%2C%20up%20to%2040,at%20least%20USD%2070%20billion">FAO Plant Production and Protection</a></li>
      </ol>
    </section>
  </div>

  <script src="../assets/script.js"></script>
  <script>
    function toggleTheme() {
      const currentTheme = document.documentElement.getAttribute('data-theme');
      const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
      document.documentElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);
    }
    document.addEventListener('DOMContentLoaded', () => {
      const savedTheme = localStorage.getItem('theme');
      if (savedTheme) {
        document.documentElement.setAttribute('data-theme', savedTheme);
      }
    });
  </script>
</body>
</html>
